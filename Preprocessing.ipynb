{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import hashlib\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_coref_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2Word = dict()\n",
    "def getID(s):\n",
    "    uid = hashlib.md5(s.encode()).hexdigest()[-6:]\n",
    "    id2Word[uid] = s\n",
    "    return uid\n",
    "\n",
    "def clean_text(text):\n",
    "    return text.replace(\"\\n\", \" \")\n",
    "\n",
    "def resolve_coref(text):\n",
    "    \"\"\"\n",
    "    :param text String of text where to resolve coreference.\n",
    "    :return: A text String where the coreference has been resolved.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    return doc._.coref_resolved    \n",
    "\n",
    "def get_entities(sent):\n",
    "    \"\"\"\n",
    "    :param sent: A parsed sentence by Spacy\n",
    "    :return: all the entities present in the sentence\n",
    "    \"\"\"\n",
    "    return [ent for ent in sent.ents if ent.label_ in ['PERSON', 'ORG', 'DATE', 'NORP', 'EVENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.txt', 'r') as myfile:\n",
    "    text=myfile.read()#.replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = clean_text(text)\n",
    "text = resolve_coref(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = []\n",
    "for sentence in nlp(text).sents:\n",
    "    sentence = sentence.as_doc()\n",
    "    sent = \" \".join([str(t) for t in sentence])\n",
    "    ents = get_entities(sentence)\n",
    "    for e1 in ents:\n",
    "        for e2 in ents:\n",
    "            if e1 != e2:\n",
    "                json_sent = {\n",
    "                    'sentence': sent,\n",
    "                    'head': {'word': e1.text, 'id': getID(e1.text)},\n",
    "                    'tail': {'word': e2.text, 'id': getID(e2.text)},\n",
    "                    'relation': 'NA' #set to NA as it has to be disovered\n",
    "                }\n",
    "                lines.append(json_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(data,outfile=\"test.json\"):\n",
    "    import json\n",
    "    with open('test.json', 'w') as outfile:\n",
    "        json.dump(data, outfile)\n",
    "write(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(id2Word, open('id2Word.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickle.load(open('id2Word.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
