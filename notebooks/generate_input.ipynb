{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to generate a `.json` file input for the model from a text, to predict the relations present in that text. In Dathena pipeline, this should be implemented in `Scala` using the output of the NER tagger. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import hashlib\n",
    "import os\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Bastien was born in New York.\n",
    "Andrew works for Apple. inc since 2010.\n",
    "Peter is the son of Robert. Peter was born in 1993.\n",
    "Peter graduated from MIT in 2019. \n",
    "Peter and his brother Andrew are French.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_sentence(text):\n",
    "    doc = nlp(text)\n",
    "    return [str(sent) for sent in doc.sents]\n",
    "sents = split_by_sentence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_hash(obj):\n",
    "    return hashlib.sha224(str(obj).encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2ent = {}\n",
    "def detect_entities(sent):\n",
    "    json_obj = []\n",
    "    doc = nlp(sent)\n",
    "    ents = [ent for ent in doc.ents  if ent.text not in ['\\n', ' ']]\n",
    "    for ent in ents:\n",
    "        for ent2 in ents:\n",
    "            if ent != ent2:\n",
    "                id2ent[str(encode_hash(ent))] = str(ent)\n",
    "                id2ent[str(encode_hash(ent2))] = str(ent2)\n",
    "                json_obj.append({'sentence': sent,\n",
    "                                'head': {\n",
    "                                    'word': str(ent),\n",
    "                                    'id': str(encode_hash(ent))\n",
    "                                } ,\n",
    "                                 'tail': {\n",
    "                                     'word': str(ent2),\n",
    "                                     'id': str(encode_hash(ent2))\n",
    "                                 },\n",
    "                                'relation': 'NA'})\n",
    "    return json_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_obj = []\n",
    "for sent in sents:\n",
    "    json_obj += detect_entities(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('id2ent.json', 'w') as f:\n",
    "    f.write(json.dumps(id2ent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../data/nyt'\n",
    "if not os.path.exists(directory):\n",
    "    os.mkdir(directory)\n",
    "with open('../data/nyt/pred.json', 'w') as f:\n",
    "    f.write(json.dumps(json_obj))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
